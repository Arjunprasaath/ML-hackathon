{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import io\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"API KEY\"\n",
    "client = OpenAI(api_key = api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "import pymupdf\n",
    "import io\n",
    "import base64\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Function to convert PDF to images\n",
    "def convert_pdf_to_images(pdf_path):\n",
    "    pdf_document = pymupdf.open(pdf_path)\n",
    "    images = []\n",
    "    for page_number in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_number)\n",
    "        pix = page.get_pixmap(matrix=pymupdf.Matrix(0.6, 0.7))  # Increase resolution\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        images.append(img)\n",
    "    pdf_document.close()\n",
    "    return images\n",
    "\n",
    "# Function to encode image to base64\n",
    "def encode_image(image):\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "# Function to analyze images\n",
    "def analyze_images(images, question):\n",
    "    responses = []\n",
    "    base64_images = [encode_image(img) for img in images]\n",
    "    for idx, img in tqdm(enumerate(base64_images)):\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": question},\n",
    "                    *[{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{img}\"}}]\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        client = OpenAI(api_key = \"API KEY\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages,\n",
    "            max_tokens=700\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "    return responses\n",
    "\n",
    "# Function to process PDF and return analysis\n",
    "def process_pdf(pdf_file):\n",
    "    if pdf_file is not None:\n",
    "        # Ensure the 'KD' directory exists\n",
    "        os.makedirs('KD', exist_ok=True)\n",
    "\n",
    "        # Determine the text file path\n",
    "        txt_file_path = os.path.join('KD', os.path.basename(pdf_file.name).replace('.pdf', '.txt'))\n",
    "\n",
    "        # Check if the text file already exists\n",
    "        if os.path.exists(txt_file_path):\n",
    "            # Read the existing text file\n",
    "            with open(txt_file_path, 'r') as file:\n",
    "                analysis = file.read()\n",
    "        else:\n",
    "            # Convert PDF to images and analyze\n",
    "            images = convert_pdf_to_images(pdf_file.name)\n",
    "            question = \"You are a Soccer Analyst, capable of reproducing data from an game's image to text in an elaborate and detailed manner. Provide a detailed description of the contents on the page. Be careful with spellings.\"\n",
    "            analysis = analyze_images(images, question)\n",
    "\n",
    "            # Save the analysis to a text file\n",
    "            with open(txt_file_path, 'w') as file:\n",
    "                file.write(\"\\n\\n\".join(analysis))\n",
    "\n",
    "        return analysis\n",
    "    else:\n",
    "        return \"Please upload a PDF file.\"\n",
    "\n",
    "# Function to handle chat interaction\n",
    "def chat_with_llm(analysis, user_query):\n",
    "    if not analysis:\n",
    "        return \"Please upload a PDF file and wait for processing to complete.\"\n",
    "\n",
    "    # Prepare messages for the OpenAI model\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": analysis},\n",
    "        {\"role\": \"user\", \"content\": user_query},\n",
    "    ]\n",
    "\n",
    "    # Call the OpenAI model\n",
    "    client = OpenAI(api_key = \"API KEY\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        max_tokens=300\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Gradio interface setup\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# PDF Analysis with LLM\")\n",
    "    gr.Markdown(\"This project allows you to upload a PDF file and interact with a language model to analyze the contents of the PDF.\")\n",
    "    \n",
    "    pdf_input = gr.File(label=\"Upload PDF\", type=\"filepath\")\n",
    "    analysis_state = gr.State()\n",
    "    user_query = gr.Textbox(label=\"Enter your query\", lines=2, interactive=False)\n",
    "    chat_output = gr.Textbox(label=\"Chat with LLM\", lines=10)\n",
    "    submit_button = gr.Button(\"Submit\", interactive=False)\n",
    "\n",
    "    def update_analysis(pdf_file):\n",
    "        analysis = process_pdf(pdf_file)\n",
    "        return analysis, gr.update(interactive=True), gr.update(interactive=True)\n",
    "\n",
    "    pdf_input.change(update_analysis, inputs=pdf_input, outputs=[analysis_state, user_query, submit_button])\n",
    "    submit_button.click(chat_with_llm, inputs=[analysis_state, user_query], outputs=chat_output)\n",
    "\n",
    "# Launch the Gradio interface\n",
    "demo.launch(server_port=7102)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arjunprasaath/Projects/socai/venv/lib/python3.11/site-packages/gradio/components/chatbot.py:225: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7033\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7033/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<PIL.Image.Image image mode=RGB size=358x590 at 0x118586890>, <PIL.Image.Image image mode=RGB size=358x590 at 0x118584B10>, <PIL.Image.Image image mode=RGB size=358x590 at 0x1182E7510>, <PIL.Image.Image image mode=RGB size=358x590 at 0x1182E6CD0>, <PIL.Image.Image image mode=RGB size=358x590 at 0x118FC65D0>, <PIL.Image.Image image mode=RGB size=358x590 at 0x1134E3910>, <PIL.Image.Image image mode=RGB size=358x590 at 0x118586810>, <PIL.Image.Image image mode=RGB size=358x590 at 0x118586A90>, <PIL.Image.Image image mode=RGB size=358x590 at 0x113520DD0>, <PIL.Image.Image image mode=RGB size=358x590 at 0x118584150>, <PIL.Image.Image image mode=RGB size=358x590 at 0x118A168D0>, <PIL.Image.Image image mode=RGB size=358x590 at 0x118584450>, <PIL.Image.Image image mode=RGB size=358x590 at 0x118587890>, <PIL.Image.Image image mode=RGB size=358x590 at 0x118FC6850>, <PIL.Image.Image image mode=RGB size=358x590 at 0x118585290>, <PIL.Image.Image image mode=RGB size=358x590 at 0x1135A49D0>, <PIL.Image.Image image mode=RGB size=358x590 at 0x118205390>, <PIL.Image.Image image mode=RGB size=358x590 at 0x1180A4C90>, <PIL.Image.Image image mode=RGB size=358x590 at 0x118584410>, <PIL.Image.Image image mode=RGB size=358x590 at 0x1182F2D90>, <PIL.Image.Image image mode=RGB size=358x590 at 0x11806C650>, <PIL.Image.Image image mode=RGB size=358x590 at 0x11854BF50>, <PIL.Image.Image image mode=RGB size=358x590 at 0x11806D410>, <PIL.Image.Image image mode=RGB size=358x590 at 0x118078290>, <PIL.Image.Image image mode=RGB size=358x590 at 0x118586950>]\n",
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 21684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 66736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:07,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 96400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:40, 17.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 89700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:55, 16.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 77124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:09, 15.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 60304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [01:22, 14.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 38544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [01:28, 11.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 83452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [01:42, 12.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 61832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [01:48, 10.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 38180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:59, 10.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 82800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [02:10, 10.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 54656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [02:20, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 49648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [02:33, 11.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 77976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [02:46, 11.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 73100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [03:11, 15.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 78080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [03:34, 17.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 126160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [04:02, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 89088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [04:21, 20.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 66864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [04:33, 18.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 56672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [04:43, 15.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 76140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [04:57, 15.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 58420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [05:02, 11.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 111452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [05:15, 12.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 75192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [05:20, 10.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 67960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [05:33, 13.34s/it]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "import pymupdf\n",
    "import io\n",
    "import base64\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import ipyplot\n",
    "\n",
    "# Global variable to store the analysis\n",
    "pdf_analysis = \"\"\n",
    "\n",
    "# Function to convert PDF to images\n",
    "def convert_pdf_to_images(pdf_path):\n",
    "    pdf_document = pymupdf.open(pdf_path)\n",
    "    images = []\n",
    "    for page_number in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_number)\n",
    "        pix = page.get_pixmap(matrix=pymupdf.Matrix(0.6, 0.7))  # Increase resolution\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        images.append(img)\n",
    "    pdf_document.close()\n",
    "    return images\n",
    "\n",
    "# Function to encode image to base64\n",
    "def encode_image(image):\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "# Function to analyze images\n",
    "def analyze_images(images, question):\n",
    "    responses = []\n",
    "    base64_images = [encode_image(img) for img in images]\n",
    "    for idx, img in tqdm(enumerate(base64_images)):\n",
    "\n",
    "        print(idx, len(img))\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"data:image/png;base64,{img}\"\n",
    "            }\n",
    "        ]\n",
    "        client = OpenAI(api_key = \"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\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "    return responses\n",
    "\n",
    "# Function to process PDF and return analysis\n",
    "def process_pdf(pdf_file):\n",
    "    if pdf_file is not None:\n",
    "        # Ensure the 'KD' directory exists\n",
    "        os.makedirs('KD', exist_ok=True)\n",
    "\n",
    "        # Determine the text file path\n",
    "        txt_file_path = os.path.join('KD', os.path.basename(pdf_file.name).replace('.pdf', '.txt'))\n",
    "\n",
    "        # Check if the text file already exists\n",
    "        if os.path.exists(txt_file_path):\n",
    "            # Read the existing text file\n",
    "            with open(txt_file_path, 'r') as file:\n",
    "                analysis = file.read()\n",
    "        else:\n",
    "            # Convert PDF to images and analyze\n",
    "            images = convert_pdf_to_images(pdf_file.name)\n",
    "            print(images)#ipyplot.plot_images(images)\n",
    "            prompt = \"You are a Soccer Analyst, capable of reproducing data from an game's image to text in an elaborate and detailed manner. Provide a detailed description of the contents on the page. Be careful with spellings, names, dates and numbers.\"\n",
    "            print(len(images))\n",
    "            analysis = analyze_images(images, prompt)\n",
    "\n",
    "            # Save the analysis to a text file\n",
    "            with open(txt_file_path, 'w') as file:\n",
    "                file.write(\"\\n\\n\".join(analysis))\n",
    "\n",
    "        return analysis\n",
    "    else:\n",
    "        return \"Please upload a PDF file.\"\n",
    "\n",
    "def chat_with_llm(message, history):\n",
    "    global pdf_analysis\n",
    "    if not pdf_analysis:\n",
    "        return [(\"Human\", message), (\"Assistant\", \"Please upload a PDF file before chatting.\")], \"\"\n",
    "    \n",
    "    # Ensure pdf_analysis is a single string\n",
    "    if isinstance(pdf_analysis, list):\n",
    "        pdf_analysis_content = \"\\n\\n\".join(pdf_analysis)\n",
    "    else:\n",
    "        pdf_analysis_content = pdf_analysis\n",
    "\n",
    "    # Prepare messages for the OpenAI model\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a Soccer Analyst assisting with detailed game data extracted from PDF documents. Use the follwoing data about the game and answer user question. CONTEXT: {pdf_analysis_content} \"\n",
    "        },\n",
    "        *[{\"role\": \"user\" if h[0] == \"Human\" else \"assistant\", \"content\": h[1]} for h in history],\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Initialize the OpenAI client\n",
    "    client = OpenAI(api_key=\"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\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=messages,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        bot_message = response.choices[0].message.content\n",
    "        history.append((\"Human\", message))\n",
    "        history.append((\"Assistant\", bot_message))\n",
    "        return history, \"\"  # Return updated history and clear input\n",
    "    except client.error.OpenAIError as e:\n",
    "        # Handle exceptions from OpenAI\n",
    "        error_message = f\"An error occurred: {e}\"\n",
    "        history.append((\"Assistant\", error_message))\n",
    "        return history, \"\"\n",
    "\n",
    "\n",
    "def handle_pdf_upload(pdf_file):\n",
    "    global pdf_analysis\n",
    "    if pdf_file is not None:\n",
    "        pdf_analysis = process_pdf(pdf_file)\n",
    "        return \"PDF processed successfully. You can now start chatting!\"\n",
    "    return \"Please upload a PDF file.\"\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# PDF Analysis with LLM\")\n",
    "    gr.Markdown(\"Upload a PDF file to analyze its contents and chat about it.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        pdf_input = gr.File(label=\"Upload PDF\", type=\"filepath\")\n",
    "        pdf_status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "\n",
    "    chatbot = gr.Chatbot(height=400)\n",
    "    msg = gr.Textbox(placeholder=\"Enter your query\", label=\"User Input\")\n",
    "    clear = gr.Button(\"Clear Chat\")\n",
    "\n",
    "    def disable_chat_input():\n",
    "        return gr.update(interactive=False), gr.update(interactive=False)\n",
    "\n",
    "    def enable_chat_input():\n",
    "        return gr.update(interactive=True), gr.update(interactive=True)\n",
    "\n",
    "    pdf_input.upload(handle_pdf_upload, inputs=pdf_input, outputs=pdf_status).then(\n",
    "        enable_chat_input, outputs=[msg, clear]\n",
    "    )\n",
    "    msg.submit(chat_with_llm, inputs=[msg, chatbot], outputs=[chatbot, msg])\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "    # Disable chat input initially\n",
    "    msg.interactive = False\n",
    "    clear.interactive = False\n",
    "\n",
    "demo.launch(server_port=7033)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Who are the substitute players from Indiana and did they score\"\n",
    "\"How did player 17 from northwestern perfrom\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arjunprasaath/Projects/socai/venv/lib/python3.11/site-packages/gradio/components/chatbot.py:225: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7042\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7042/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted PDF to 24 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:   0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 1/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:   4%|▍         | 1/24 [00:04<01:42,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 2/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:   8%|▊         | 2/24 [00:14<02:56,  8.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 3/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  12%|█▎        | 3/24 [00:33<04:27, 12.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 4/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  17%|█▋        | 4/24 [00:43<03:56, 11.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 5/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  21%|██        | 5/24 [01:02<04:32, 14.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 6/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  25%|██▌       | 6/24 [01:10<03:39, 12.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 7/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  29%|██▉       | 7/24 [01:14<02:38,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 8/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  33%|███▎      | 8/24 [01:25<02:40, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 9/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  38%|███▊      | 9/24 [01:47<03:27, 13.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 10/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  42%|████▏     | 10/24 [01:57<02:56, 12.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 11/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  46%|████▌     | 11/24 [02:08<02:36, 12.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 12/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  50%|█████     | 12/24 [02:12<01:53,  9.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 13/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  54%|█████▍    | 13/24 [02:15<01:25,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 14/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  58%|█████▊    | 14/24 [02:21<01:12,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 15/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  62%|██████▎   | 15/24 [02:31<01:11,  7.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 16/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  67%|██████▋   | 16/24 [02:38<01:01,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 17/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  71%|███████   | 17/24 [02:54<01:10, 10.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 18/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  75%|███████▌  | 18/24 [03:07<01:05, 10.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 19/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  79%|███████▉  | 19/24 [03:14<00:49,  9.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 20/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  83%|████████▎ | 20/24 [03:26<00:41, 10.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 21/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  88%|████████▊ | 21/24 [03:40<00:34, 11.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 22/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  92%|█████████▏| 22/24 [03:49<00:21, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 23/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images:  96%|█████████▌| 23/24 [03:56<00:09,  9.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 24/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Images: 100%|██████████| 24/24 [04:00<00:00, 10.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image analysis completed.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "import pymupdf\n",
    "import io\n",
    "import base64\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Ensure OpenAI API Key is set in environment variables\n",
    "API_KEY = \"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\"\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"OpenAI API key not found. Please set the OPENAI_API_KEY environment variable.\")\n",
    "\n",
    "# Set OpenAI API key\n",
    "client = OpenAI(api_key = API_KEY)\n",
    "\n",
    "# Global variable to store the analysis\n",
    "pdf_analysis = \"\"\n",
    "\n",
    "# Function to convert PDF to images\n",
    "def convert_pdf_to_images(pdf_path):\n",
    "    pdf_document = pymupdf.open(pdf_path)\n",
    "    images = []\n",
    "    for page_number in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_number)\n",
    "        pix = page.get_pixmap(matrix=pymupdf.Matrix(0.6, 0.7))  # Adjust resolution as needed\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        images.append(img)\n",
    "    pdf_document.close()\n",
    "    return images\n",
    "\n",
    "# Function to encode image to base64\n",
    "def encode_image(image):\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "# Function to analyze images\n",
    "def analyze_images(images, question, log):\n",
    "    responses = []\n",
    "    base64_images = [encode_image(img) for img in images]\n",
    "    \n",
    "    for idx, img in tqdm(enumerate(base64_images), total=len(base64_images), desc=\"Analyzing Images\"):\n",
    "        log += f\"Processing image {idx + 1}/{len(base64_images)}\\n\"\n",
    "        print(f\"Processing image {idx + 1}/{len(base64_images)}\")\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"data:image/png;base64,{img}\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        #try:\n",
    "        client = OpenAI(api_key = API_KEY)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        analysis_text = response.choices[0].message.content.strip()\n",
    "        responses.append(analysis_text)\n",
    "        log += f\"Image {idx + 1} processed successfully.\\n\"\n",
    "        # except openai.error.OpenAIError as e:\n",
    "        #     error_msg = f\"Error processing image {idx + 1}: {e}\\n\"\n",
    "        #     responses.append(error_msg)\n",
    "        #     log += error_msg\n",
    "    \n",
    "    return responses, log\n",
    "\n",
    "# Function to summarize analysis (Optional but recommended)\n",
    "def summarize_analysis(full_text):\n",
    "    # try:\n",
    "    client = OpenAI(api_key = API_KEY)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a summarization assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Please provide a concise summary of the following text:\\n\\n{full_text}\"}\n",
    "        ],\n",
    "        max_tokens=500\n",
    "    )\n",
    "    summary = response.choices[0].message.content.strip()\n",
    "    return summary\n",
    "    # except openai.error.OpenAIError as e:\n",
    "    #     return f\"Error during summarization: {e}\"\n",
    "\n",
    "# Function to process PDF and return analysis\n",
    "def process_pdf(pdf_file, log):\n",
    "    global pdf_analysis\n",
    "    if pdf_file is not None:\n",
    "        # Ensure the 'KD' directory exists\n",
    "        os.makedirs('KD', exist_ok=True)\n",
    "        # Determine the text file path\n",
    "        txt_file_path = os.path.join('KD', os.path.basename(pdf_file.name).replace('.pdf', '.txt'))\n",
    "        \n",
    "        # Check if the text file already exists\n",
    "        if os.path.exists(txt_file_path):\n",
    "            # Read the existing text file\n",
    "            with open(txt_file_path, 'r', encoding='utf-8') as file:\n",
    "                analysis = file.read()\n",
    "            log += \"Existing analysis found. Loading from file.\\n\"\n",
    "        else:\n",
    "            # Convert PDF to images and analyze\n",
    "            images = convert_pdf_to_images(pdf_file.name)\n",
    "            log += f\"Converted PDF to {len(images)} images.\\n\"\n",
    "            print(f\"Converted PDF to {len(images)} images.\")\n",
    "            \n",
    "            prompt = (\"You are a Soccer Analyst, capable of reproducing data from an game's image to text in an elaborate and detailed manner. \"\n",
    "                      \"Provide a detailed description of the contents on the page. Be careful with spellings, names, dates, and numbers.\")\n",
    "            \n",
    "            analysis, log = analyze_images(images, prompt, log)\n",
    "            log += \"Image analysis completed.\\n\"\n",
    "            print(\"Image analysis completed.\")\n",
    "            \n",
    "            # Optional: Summarize the analysis to fit within token limits\n",
    "            # summarized_analysis = summarize_analysis(\"\\n\\n\".join(analysis))\n",
    "            # if not summarized_analysis.startswith(\"Error during summarization\"):\n",
    "            #     analysis = summarized_analysis\n",
    "            #     log += \"Summarization completed successfully.\\n\"\n",
    "            # else:\n",
    "            #     log += f\"{summarized_analysis}\\n\"\n",
    "            \n",
    "            # Save the analysis to a text file\n",
    "\n",
    "            with open(txt_file_path, 'w', encoding='utf-8') as file:\n",
    "                if isinstance(analysis, list):\n",
    "                    file.write(\"\\n\\n\".join(analysis))\n",
    "                else:\n",
    "                    file.write(analysis)\n",
    "            # with open(txt_file_path, 'w', encoding='utf-8') as file:\n",
    "            #     file.write(analysis)\n",
    "            log += f\"Analysis saved to {txt_file_path}.\\n\"\n",
    "        \n",
    "        pdf_analysis = analysis  # Store the analysis for chatbot\n",
    "        return \"PDF processed successfully. You can now start chatting!\", log\n",
    "    else:\n",
    "        log += \"No PDF file uploaded.\\n\"\n",
    "        return \"Please upload a PDF file.\", log\n",
    "\n",
    "# Function to handle chat with LLM\n",
    "def chat_with_llm(message, history, log):\n",
    "    global pdf_analysis\n",
    "    if not pdf_analysis:\n",
    "        log += \"Chat attempted without PDF analysis.\\n\"\n",
    "        return [(\"Human\", message), (\"Assistant\", \"Please upload a PDF file before chatting.\")], \"\", log\n",
    "    \n",
    "    # Ensure pdf_analysis is a single string\n",
    "    if isinstance(pdf_analysis, list):\n",
    "        pdf_analysis_content = \"\\n\\n\".join(pdf_analysis)\n",
    "    else:\n",
    "        pdf_analysis_content = pdf_analysis\n",
    "    \n",
    "    # Prepare messages for the OpenAI model\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a Soccer Analyst assisting with detailed game data extracted from PDF documents. Use the following data about the game to answer the user's questions.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"CONTEXT: {pdf_analysis_content}\"\n",
    "        },\n",
    "        *[{\"role\": \"user\" if h[0] == \"Human\" else \"assistant\", \"content\": h[1]} for h in history],\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    bot_message = response.choices[0].message.content.strip()\n",
    "    history.append((\"Human\", message))\n",
    "    history.append((\"Assistant\", bot_message))\n",
    "    log += f\"User: {message}\\nAssistant: {bot_message}\\n\"\n",
    "    return history, \"\", log\n",
    "    # except openai.error.OpenAIError as e:\n",
    "    #     error_message = f\"An error occurred: {e}\"\n",
    "    #     history.append((\"Assistant\", error_message))\n",
    "    #     log += f\"{error_message}\\n\"\n",
    "    #     return history, \"\", log\n",
    "\n",
    "# Function to handle PDF upload\n",
    "def handle_pdf_upload(pdf_file, log):\n",
    "    if pdf_file is not None:\n",
    "        status, log = process_pdf(pdf_file, log)\n",
    "        return status, log\n",
    "    else:\n",
    "        log += \"PDF upload failed. No file provided.\\n\"\n",
    "        return \"Please upload a PDF file.\", log\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# PDF Analysis with LLM\")\n",
    "    gr.Markdown(\"Upload a PDF file to analyze its contents and chat about it.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        pdf_input = gr.File(label=\"Upload PDF\", type=\"filepath\")  # Corrected type\n",
    "        pdf_status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "    \n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=400)\n",
    "        msg = gr.Textbox(placeholder=\"Enter your query\", label=\"User Input\")\n",
    "        clear = gr.Button(\"Clear Chat\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        log_box = gr.Textbox(label=\"Logging\", lines=10, interactive=False)\n",
    "    \n",
    "    def disable_chat_input():\n",
    "        return gr.update(interactive=False), gr.update(interactive=False)\n",
    "    \n",
    "    def enable_chat_input():\n",
    "        return gr.update(interactive=True), gr.update(interactive=True)\n",
    "    \n",
    "    # Initially disable chat input and clear button\n",
    "    msg.interactive = False\n",
    "    clear.interactive = False\n",
    "    \n",
    "    # Handle PDF upload\n",
    "    pdf_input.upload(\n",
    "        fn=handle_pdf_upload,\n",
    "        inputs=[pdf_input, log_box],\n",
    "        outputs=[pdf_status, log_box]\n",
    "    ).then(\n",
    "        enable_chat_input,\n",
    "        outputs=[msg, clear]\n",
    "    )\n",
    "    \n",
    "    # Handle user messages\n",
    "    msg.submit(\n",
    "        fn=chat_with_llm,\n",
    "        inputs=[msg, chatbot, log_box],\n",
    "        outputs=[chatbot, msg, log_box]\n",
    "    )\n",
    "    \n",
    "    # Handle clearing the chat\n",
    "    clear.click(\n",
    "        fn=lambda: ([], \"\", \"Chat cleared.\\n\"),\n",
    "        inputs=None,\n",
    "        outputs=[chatbot, msg, log_box],\n",
    "        queue=False\n",
    "    )\n",
    "    \n",
    "    # Ensure the chatbot input is disabled initially\n",
    "    pdf_status.value = \"Please upload a PDF file to begin.\"\n",
    "    \n",
    "demo.launch(server_port=7042)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
